---
layout: post
title:  "分布式系统 - replication"
date:   2018-02-06 23:11:00 +0800
---

在看一本很棒的书，《Designing Data-Intensive Applications》。可以说是很多分布式问题的导论了。接下来会有一系列的文章结合书中内容，说说我的归纳、总结和一些思考。由于是一本关于分布式数据库的书，所以里面很多例子会以数据库领域遇到的问题作为引子，但是大部分分布式系统要解决的问题都是类似的。首先，来说说复制（replication）。

复制被认为是容错和提高可用性的有效手段。目前这一技术已经比较成熟，我们先会看看这一技术需要考虑的点，一致性问题，然后再说说具体的实施方案（一主多从、多主复制等）。

### 复制技术的常见问题

首先来看看，如果要你实现一个有replication的分布式系统，需要考虑什么问题。（当然，不到万不得已，不要这么做。）

我们以一个极其常见的复制系统为例子：一主多从的mysql集群：

1. 有master和slave节点
2. 写操作只能发生在master节点
3. slave的数据和master的数据满足最终一致性
4. 每次master写入成功后，通过发送replication log或change stream给slave节点，实现复制。

#### Synchronous VS Asynchronous

一个要考虑的点是：同步复制还是异步复制，如图：

![Alt](/images/replication-1.jpg)

如上图，leader和follower 1之间使用的是同步复制，它需要等复制成功后，才告诉客户端，写入成功。leader和follower 2之间使用的是异步复制，它在给客户端返回成功之后，才对follower 2进行复制操作。

不难看出同步复制的优缺点：

- 优点：由于一次写操作必须同时写入leader和follower 1才算是成功，所以同步复制是可以保证从节点和主节点的一致性，且保证从节点的数据总是最新的。
- 缺点同样明显：一次写操作必须同时写入主从才算成功，如果主节点可用，但从节点不可用，或者有网络抖动，写入都会失败。这其实是降低了可用性。

用CAP理论解释一下，是选择了C（Consistency），一定程度的放弃了A（Available）。

在实际场景中，不会选择全同步复制（那将会大大降低系统的可用性和scalability）。而会选择半同步（semi-synchronous）复制。让客户指定多少个从节点是同步复制，多少个从节点异步复制。（通常是可配置的）

异步复制看起来，在性能和拓展性方面表现比较好，而缺点是：它的durability不如同步复制。如果主节点在同步从节点的时候挂了，且不能恢复，而从节点的数据还没追平主节点，这些数据就会丢失了。

#### 部署从节点

对于有状态的集群，部署一个节点不会像无状态集群那么简单。需要考虑的是这个节点如果追平整个集群的状态，又在整个过程中让整个集群保持可用。一个套路用snapshot的方式，步骤是这样的：

1. 在部署之前，对master节点拍一个快照（snapshot）。在很多实现里，这是不需要锁库的。
2. 将这个snapshot发送给新的节点。
3. 新节点有了snapshot的状态之后，向master节点请求这段时间差的replication log。
4. 当新节点追平了replication log，就可以上线了。

从节点的故障恢复和部署是类似的，如果replication log里有偏序信息，从节点就能知道故障发生的开始时间，它从这个时间点开始请求master节点的replication log，重复步骤3、4。

#### Master failover

master的故障要更难搞一些：如果master节点挂了，为了是集群保持可用，必须要有failover手段：在其他从节点里，找到一个，并要求它成为master节点。

一次自动的failover会是这样：

1. Determining that the leader has failed. 常见的方法是节点之间保持心跳，当多个节点检测到主节点心跳异常时，认为主节点不可用。
2. Choosing a new leader. 有两种方法可以选择新的master节点。一个是用选举的方式，或者由一个控制节点（control node）指定。如果使用第二种方法的话，集群里就需要新增一种角色（控制节点）。两种方法的目标都是一样的，选择数据上和master节点最接近的从节点，使数据丢失最小化。
3. Reconfiguring the system to use the new leader. 当完成选择了新的master节点后，将流量引导到新的master节点上。

整个过程看似并不复杂，实际实现起来要麻烦很多：

1. 当原来的master挂了之后，新的master节点有可能没有和原master的数据完全同步。那么当原master恢复，重新加入到集群时，会发生conflict writes。所以，一个常见的做法是：只要master挂了，直接t掉，不会再恢复加入到集群。
2. 直接放弃原master节点的问题是：在原master中，但没有复制到从节点的数据，也会被放弃。那么问题是，有可能由于数据丢失造成数据不可用甚至泄密。比如：id为13的一行由于master节点的failover被丢弃了，在新的master中重新使用了13作为nextval。造成数据异常。
3. 在特定的场景里，有可能在第二步（Choosing a new leader）时，选出了两个master节点，那么整个集群就会陷入split brain的困局。一般的一主多从数据库都不会有write conflict的处理，那么两个master对同一条数据写入，就会导致数据不准确。
4. 使用心跳鉴别master是否在异常情况，同样有危险的陷阱：一般的心跳+timeout的做法，究竟timeout的时间设在多少比较合适呢？如果timeout设置的过小，造成的问题是，一些比较严重的网络抖动会使误认为是master故障，更有甚者，严重的网络抖动使整个集群不停的执行master failover，会使整个集群不可用。

所以在实际场景里，使用自动failover还是人肉failover也是一个trade-off。

### 一致性问题

根据CAP理论，不难证明，复制必然会带来一致性问题。一般应用的复制都会使用异步复制，异步复制的一致性保证是最终一致的。也就是主从节点之间会有数据不一致的时间窗口，且这个时间窗口的大小是没有限制的，当网络或集群状态抖动时，不一致的时间窗口会增大，我们来看看这时候，会产生什么问题。

#### read your own writes

![Alt](/images/replication-2.jpg)

如图，在成功写入数据之后，在复制到从节点之前，如果在这时读取同一条记录，那么有可能读取不到刚刚写入的改动。

出现这样的问题，意味着我们无法将数据库集群当成一个single node看待。那么只能在集群外部做一些优化，比如：

1. 最简单的做法，读主库。
2. 但如果实际场景是，数据总是被频繁修改，且总是希望短时间内能读到最近的更新，还用方法1的话，就会牺牲scalability。

### 参考

- 《Designing Data-Intensive Applications》
